{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAI9m7jrjjUN",
        "outputId": "6dc5e5b9-e4ba-4fa5-b393-4e4da5397c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Starting parameters ===\n",
            "FC1 weights:\n",
            " tensor([[ 0.2000, -0.2000],\n",
            "        [-0.2000,  0.2000],\n",
            "        [-0.2000, -0.2000]])\n",
            "FC2 weights:\n",
            " tensor([[ 0.5000, -0.5000,  0.5000],\n",
            "        [ 0.5000,  0.5000,  0.5000],\n",
            "        [ 0.5000,  0.5000, -0.5000]])\n",
            "FC3 weights:\n",
            " tensor([[-0.8000,  0.8000,  0.8000],\n",
            "        [ 0.8000, -0.8000,  0.8000]])\n",
            "———————————————\n",
            "\n",
            "=== Iteration 0: forward/backward ===\n",
            "After fc1_pre:\n",
            " [[-0.04  0.04 -0.16]]\n",
            "After fc1_relu:\n",
            " [[0.   0.04 0.  ]]\n",
            "After fc2_pre:\n",
            " [[-0.02  0.02  0.02]]\n",
            "After fc2_relu:\n",
            " [[0.   0.02 0.02]]\n",
            "After fc3_pre (logits):\n",
            " [[0.032 0.   ]]\n",
            "Softmax probs:\n",
            " [[0.5079993  0.49200067]]\n",
            "\n",
            "=== Gradients of every intermediate activation ===\n",
            "fc1_pre.grad =\n",
            " tensor([[ 0.0000, -0.3936,  0.0000]])\n",
            "fc1_post.grad =\n",
            " tensor([[-0.3936, -0.3936, -0.3936]])\n",
            "fc2_pre.grad =\n",
            " tensor([[ 0.0000e+00, -7.8720e-01, -1.2016e-08]])\n",
            "fc2_post.grad =\n",
            " tensor([[ 7.8720e-01, -7.8720e-01, -1.2016e-08]])\n",
            "fc3_pre.grad =\n",
            " tensor([[-0.4920,  0.4920]])\n",
            "\n",
            "=== After SGD step ===\n",
            "FC3 weights:\n",
            " tensor([[-0.8000,  0.8098,  0.8098],\n",
            "        [ 0.8000, -0.8098,  0.7902]])\n",
            "FC2 weights:\n",
            " tensor([[ 0.5000, -0.5000,  0.5000],\n",
            "        [ 0.5000,  0.5315,  0.5000],\n",
            "        [ 0.5000,  0.5000, -0.5000]])\n",
            "FC1 weights:\n",
            " tensor([[ 0.2000, -0.2000],\n",
            "        [-0.0819,  0.3968],\n",
            "        [-0.2000, -0.2000]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class TinyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1  = nn.Linear(2, 3, bias=True)\n",
        "        self.fc2  = nn.Linear(3, 3, bias=True)\n",
        "        self.fc3  = nn.Linear(3, 2, bias=True)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # ——— init to your sketch ———\n",
        "        with torch.no_grad():\n",
        "            self.fc1.weight.data = torch.tensor([\n",
        "                [ 0.2, -0.2],\n",
        "                [-0.2,  0.2],\n",
        "                [-0.2, -0.2],\n",
        "            ])\n",
        "            self.fc1.bias.data.zero_()\n",
        "\n",
        "            self.fc2.weight.data = torch.tensor([\n",
        "                [ 0.5, -0.5,  0.5],\n",
        "                [ 0.5,  0.5,  0.5],\n",
        "                [ 0.5,  0.5, -0.5]\n",
        "            ])\n",
        "            self.fc2.bias.data.zero_()\n",
        "\n",
        "            self.fc3.weight.data = torch.tensor([\n",
        "                [-0.8,  0.8,  0.8],\n",
        "                [ 0.8, -0.8,  0.8]\n",
        "            ])\n",
        "            self.fc3.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # — flatten any extra dims so we always have (batch, 2)\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(x.size(0), -1)\n",
        "\n",
        "        # fc1 pre\n",
        "        x_fc1_pre = self.fc1(x)\n",
        "        x_fc1_pre.retain_grad()\n",
        "\n",
        "        # fc1 post\n",
        "        x_fc1 = self.relu(x_fc1_pre)\n",
        "        x_fc1.retain_grad()\n",
        "\n",
        "        # fc2 pre\n",
        "        x_fc2_pre = self.fc2(x_fc1)\n",
        "        x_fc2_pre.retain_grad()\n",
        "\n",
        "        # fc2 post\n",
        "        x_fc2 = self.relu(x_fc2_pre)\n",
        "        x_fc2.retain_grad()\n",
        "\n",
        "        # fc3 logits\n",
        "        x_fc3_pre = self.fc3(x_fc2)\n",
        "        x_fc3_pre.retain_grad()\n",
        "\n",
        "        # stash for later\n",
        "        self.activations = {\n",
        "            'fc1_pre':  x_fc1_pre,\n",
        "            'fc1_post': x_fc1,\n",
        "            'fc2_pre':  x_fc2_pre,\n",
        "            'fc2_post': x_fc2,\n",
        "            'fc3_pre':  x_fc3_pre,\n",
        "        }\n",
        "\n",
        "        # prints\n",
        "        print(\"After fc1_pre:\\n\", x_fc1_pre.detach().numpy())\n",
        "        print(\"After fc1_relu:\\n\", x_fc1.detach().numpy())\n",
        "        print(\"After fc2_pre:\\n\", x_fc2_pre.detach().numpy())\n",
        "        print(\"After fc2_relu:\\n\", x_fc2.detach().numpy())\n",
        "        print(\"After fc3_pre (logits):\\n\", x_fc3_pre.detach().numpy())\n",
        "        print(\"Softmax probs:\\n\",\n",
        "              nn.functional.softmax(x_fc3_pre, dim=-1).detach().numpy())\n",
        "\n",
        "        return x_fc3_pre\n",
        "\n",
        "# — set up\n",
        "model     = TinyNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "opt       = optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "print(\"=== Starting parameters ===\")\n",
        "print(\"FC1 weights:\\n\", model.fc1.weight.data)\n",
        "print(\"FC2 weights:\\n\", model.fc2.weight.data)\n",
        "print(\"FC3 weights:\\n\", model.fc3.weight.data)\n",
        "print(\"———————————————\\n\")\n",
        "\n",
        "inp = torch.tensor([[0.3, 0.5]], dtype=torch.float32)  # shape (1,2)\n",
        "target = torch.tensor([0])                             # shape (1,)\n",
        "\n",
        "print(\"=== Iteration 0: forward/backward ===\")\n",
        "out   = model(inp)\n",
        "loss  = criterion(out, target)\n",
        "loss.backward()\n",
        "\n",
        "print(\"\\n=== Gradients of every intermediate activation ===\")\n",
        "for name, tensor in model.activations.items():\n",
        "    print(f\"{name}.grad =\\n\", tensor.grad)\n",
        "\n",
        "opt.step()\n",
        "\n",
        "print(\"\\n=== After SGD step ===\")\n",
        "print(\"FC3 weights:\\n\", model.fc3.weight.data)\n",
        "print(\"FC2 weights:\\n\", model.fc2.weight.data)\n",
        "print(\"FC1 weights:\\n\", model.fc1.weight.data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cxoa4J2lmI-f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}